<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Our method learns to generate dexterous grasps based on 3D partial point clouds.">
  <meta name="keywords" content="dexterous grasping, diffusion model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DexDiffuser: Generating Dexterous Grasps with Diffusion Models</title>

  <meta property="og:image" content="resources/og_image.jpg"/>
	<meta property="og:title" content="DexDiffuser: Generating Dexterous Grasps with Diffusion Models" />
	<meta property="og:description" content="Our method learns to generate approach-constrained grasp poses for parallel-jaw grippers based on 3D partial point clouds." />
  <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
      if you update and want to force Twitter to re-scrape. -->
  <meta property="twitter:card"          content="summary" />
  <meta property="twitter:title"         content="DexDiffuser: Generating Dexterous Grasps with Diffusion Models" />
  <meta property="twitter:description"   content="Our method learns to generate dexterous grasps based on 3D partial point clouds." />
  <meta property="twitter:image"         content="resources/og_image.jpg" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VFNFH9CKNX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-VFNFH9CKNX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./resources/carousel-horse.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <!-- <video id="banner" autoplay muted loop playsinline height="100%">
          <source src="resources/carousel.mp4"
                  type="video/mp4">
        </video> -->
      </div>
      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title" style="font-size: 2.2rem;">DexDiffuser: Generating Dexterous Grasps with Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.kth.se/profile/zehang">Zehang Weng</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.kth.se/profile/haofeil">Haofei Lu</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.kth.se/~dani/">Danica Kragic</a>,
            </span>
            <span class="author-block">
              <a href="https://jsll.github.io">Jens Lundell</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Division of Robotics, Perception and Learning (RPL), KTH</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(* equal contribution)</span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">IEEE Robotics and Automation Letters (RA-L)</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.02989"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://youtu.be/MAJyj3V0_kI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span> -->
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/YuLiHN/DexDiffuser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="resources/dexdiff_compress_part0.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle">
        Our method can generate high-quality grasp poses for unknown objects based on 3D partial point clouds. We conducted our study in both simulation and real world.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous
            grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion and
            Evaluator-based Sampling Refinement. The experiment results demonstrate that DexDiffuser consistently outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 9.12% and 19.44% higher grasp success rate in simulation and real robot experiments, respectively
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MAJyj3V0_kI" title="CAPGrasp" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Real Robot Grasping</h2>
      <div class="publication-video">
        <div class="content has-text-centered">
          <video id="table-picking" autoplay controls muted preload loop playsinline>
            <source src="resources/dexdiff_compress_part2.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>
  <!--/ Paper video. -->

</div>

<div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Grasps for the toy plane</h2>
    <div class="publication-video">
      <div class="content has-text-centered">
        <video id="shelf-picking" autoplay controls muted preload loop playsinline>
          <source src="resources/dexdiff_compress_toyairplane.mp4"
                  type="video/mp4">
        </video>
    </div>
  </div>
</div>

</div>




</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Single-Image 3D Reconstruction</h2>

        <h2 class="title is-5">Real Horse Images and Realistic Paintings</h2>
        <div class="content has-text-justified">
          <p>
            After training, given a single image of a new instance, the model reconstructs articulated 3D shape and appearance of it, which can be animated and re-rendered from arbitrary viewpoints.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/RAL-web-video-compress.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Generalisation to Abstract Horse Drawings and Artefacts</h2>
        <div class="content has-text-justified">
          <p>
            The model also generalises to abstract drawings and artefacts, despite being trained only on real images.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/horse_abstract.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Other Animal Categories: Giraffes, Zebras and Cows</h2>
        <div class="content has-text-justified">
          <p>
            After finetuning, our model also generalises to various animal categories with highly different underlying shapes.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/other_animals.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Frame-by-Frame Reconstruction on Videos</h2>
        <div class="content has-text-justified">
          <p>
            We run the model on videos frame by frame, and obtain temporally consistent reconstructions.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline style="max-width: 86%; display: block; margin: auto;">
            <source src="resources/video_recon.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>

  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{weng2024dexdiffusergeneratingdexterousgrasps,
      title={DexDiffuser: Generating Dexterous Grasps with Diffusion Models}, 
      author={Zehang Weng and Haofei Lu and Danica Kragic and Jens Lundell},
      year={2024},
      eprint={2402.02989},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2402.02989}, }
</code></pre>
  </div>
</section>

<!-- <section class="section" id="Acknowledgements"> -->
  <!-- <div class="container is-max-desktop content"> -->
    <!-- <h2 class="title">Acknowledgements</h2> -->
    <!-- <p> -->
      <!-- We would like to thank the authors of <a href="https://github.com/NVlabs/nvdiffrec">nvdiffrec</a> for open-sourcing the code for DMTet and rendering. We are also grateful to Tengda Han, Shu Ishida, Dylan Campbell, Eldar Insafutdinov, Luke Melas-Kyriazi, Ragav Sachdeva and Sagar Vaze for insightful discussions, and Guanqi Zhan and Jaesung Huh for proofreading. -->
    <!-- </p> -->
    <!-- <p> -->
      <!-- Shangzhe Wu is supported by Meta Research. Tomas Jakab is supported by ERC-CoG UNION 101001212. Christian Rupprecht is supported by VisualAI EP/T028572/1 and ERC-CoG UNION 101001212. Andrea Vedaldi is supported by ERC-CoG UNION 101001212. -->
    <!-- </p> -->
  <!-- </div> -->
<!-- </section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
